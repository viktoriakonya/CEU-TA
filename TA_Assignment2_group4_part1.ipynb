{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Group**: Peter Endes-Nagy, Khawaja Hassan, Shah Ali, Viktoria Konya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 1**\n",
    "\n",
    "Use the text similarity notebook and do the following: 1) lemmatize the tokens, 2) change min_df. How do your results look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"This little cat came to play when I was eating at a restaurant. I had to take a photo.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tabs in google chrome you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with the current version of the google translate app.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preprocesser - Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def text_preprocesser_lemmatize(text):\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\W',' ', text)\n",
    "    \n",
    "    # Remove not alphabet characters\n",
    "    text = re.sub(\"[^a-zA-Z]+\", \" \", text)\n",
    "    \n",
    "    # Lowercase and tokenize\n",
    "    tokens = [word.lower() for word in nltk.word_tokenize(text)]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    # Remove words with length less than 3 characters\n",
    "    tokens = [token for token in tokens if len(token)>=3]\n",
    "    \n",
    "    lemma = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "    \n",
    "    # Join\n",
    "    preprocessed_text = ' '.join(lemma)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply text preprocesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['little cat come play eat restaurant take photo',\n",
       " 'merley best squooshy kitten belly',\n",
       " 'google translate app incredible',\n",
       " 'open tab google chrome get smiley face',\n",
       " 'best cat photo ever take',\n",
       " 'climb ninja cat',\n",
       " 'impressed current version google translate app',\n",
       " 'key promoter extension google chrome']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check results\n",
    "[text_preprocesser_lemmatize(document) for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply TF-IDF vectorizer\n",
    "\n",
    "We will:\n",
    "\n",
    "* ignore terms that appear in less 2 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function text_preprocesser_lemmatize at 0x0000015172BAF1E0>,\n",
       "        smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "        sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=text_preprocesser_lemmatize, min_df = 2)\n",
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>app</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623489</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.520868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445928</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrome</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.471725</td>\n",
       "      <td>0.603358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.471725</td>\n",
       "      <td>0.603358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photo</th>\n",
       "      <td>0.603613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take</th>\n",
       "      <td>0.603613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>translate</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623489</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0    1         2         3         4    5         6  \\\n",
       "app        0.000000  0.0  0.623489  0.000000  0.000000  0.0  0.623489   \n",
       "best       0.000000  1.0  0.000000  0.000000  0.516768  0.0  0.000000   \n",
       "cat        0.520868  0.0  0.000000  0.000000  0.445928  1.0  0.000000   \n",
       "chrome     0.000000  0.0  0.000000  0.797471  0.000000  0.0  0.000000   \n",
       "google     0.000000  0.0  0.471725  0.603358  0.000000  0.0  0.471725   \n",
       "photo      0.603613  0.0  0.000000  0.000000  0.516768  0.0  0.000000   \n",
       "take       0.603613  0.0  0.000000  0.000000  0.516768  0.0  0.000000   \n",
       "translate  0.000000  0.0  0.623489  0.000000  0.000000  0.0  0.623489   \n",
       "\n",
       "                  7  \n",
       "app        0.000000  \n",
       "best       0.000000  \n",
       "cat        0.000000  \n",
       "chrome     0.797471  \n",
       "google     0.603358  \n",
       "photo      0.000000  \n",
       "take       0.000000  \n",
       "translate  0.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "df = pd.DataFrame(tfidf.toarray().transpose(), index=tfidf_vectorizer.get_feature_names())\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the transformation 'taken' was merged with 'take' which increased the frequency of this token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the TF-IDF vectorizes with another upper and lower ceiling value of the frequencies.\n",
    "\n",
    "Here we will:\n",
    "\n",
    "* ignore terms that appear in less than 20% of the documents and\n",
    "* ignore terms that appear in more than 80% of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>app</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623489</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.520868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445928</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrome</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.471725</td>\n",
       "      <td>0.603358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.471725</td>\n",
       "      <td>0.603358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photo</th>\n",
       "      <td>0.603613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take</th>\n",
       "      <td>0.603613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>translate</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623489</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0    1         2         3         4    5         6  \\\n",
       "app        0.000000  0.0  0.623489  0.000000  0.000000  0.0  0.623489   \n",
       "best       0.000000  1.0  0.000000  0.000000  0.516768  0.0  0.000000   \n",
       "cat        0.520868  0.0  0.000000  0.000000  0.445928  1.0  0.000000   \n",
       "chrome     0.000000  0.0  0.000000  0.797471  0.000000  0.0  0.000000   \n",
       "google     0.000000  0.0  0.471725  0.603358  0.000000  0.0  0.471725   \n",
       "photo      0.603613  0.0  0.000000  0.000000  0.516768  0.0  0.000000   \n",
       "take       0.603613  0.0  0.000000  0.000000  0.516768  0.0  0.000000   \n",
       "translate  0.000000  0.0  0.623489  0.000000  0.000000  0.0  0.623489   \n",
       "\n",
       "                  7  \n",
       "app        0.000000  \n",
       "best       0.000000  \n",
       "cat        0.000000  \n",
       "chrome     0.797471  \n",
       "google     0.603358  \n",
       "photo      0.000000  \n",
       "take       0.000000  \n",
       "translate  0.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=text_preprocesser_lemmatize, min_df=0.2, max_df= 0.8)\n",
    "tfidf_vectorizer\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "df = pd.DataFrame(tfidf.toarray().transpose(), index=tfidf_vectorizer.get_feature_names())\n",
    "df "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
